{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this demo, want to use tranformer to do the VQA\n",
    "    As the original Transformer Decoder just need text input, but for the VQA, we still have the picture feature data. So for this, I add image feature input in my decoder, which name DecoderWithImage, which use the DecoderWithImageLayer. This new decoder layer add attention for the image feature, which after first multi-head attention layer in the normal decode layer. Then concate the image attention feature with decode input embedding, and pass those data through GRU layer, Dense layer, then concate with normal seconde multi-head layer, the feed forword network, which is same as normal DecoderLayer.\n",
    "    Do the training with VQA data, it is much better than last VQA demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#transformer \n",
    "#1. position_embedding\n",
    "#2.self attention layer\n",
    "#3.feed forward network\n",
    "\n",
    "#1. position_embedding\n",
    "#1.1 count angle degree\n",
    "def get_position_angle(position, i, dmodel):\n",
    "    angle = 1/np.power(10000, (2 *(i//2))/np.float32(dmodel))\n",
    "    return position * angle\n",
    "\n",
    "def get_position_embedding(pos, dmodel):\n",
    "    angles = get_position_angle(np.arange(pos)[:,np.newaxis], np.arange(dmodel)[np.newaxis,:], dmodel)\n",
    "    \n",
    "    angles[:,0::2] = np.sin(angles[:,0::2])\n",
    "    angles[:,1::2] = np.cos(angles[:,1::2])\n",
    "    \n",
    "    position_embedding = angles[np.newaxis, ...]\n",
    "    return tf.cast(position_embedding, dtype = tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. attention layer\n",
    "#2.1 attention layer weight\n",
    "#when weight computing formula is weight = softmax(Q.K/sqrt(depth)) , and value is weight * V\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    matmul_qk = tf.linalg.matmul(q, k, transpose_b = True)\n",
    "    \n",
    "    b_soft = matmul_qk/tf.math.sqrt(tf.cast(tf.shape(q)[-1], dtype=tf.float32))\n",
    "    \n",
    "    weights = tf.keras.backend.softmax(b_soft, axis = -1)\n",
    "    output = tf.linalg.matmul(weights, v)\n",
    "\n",
    "    return output, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. feed forward network\n",
    "def feed_forward_network(dmodel, dff):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(dff, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(dmodel))\n",
    "    return model\n",
    "    #return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(dmodel)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "#test for feed forward network\n",
    "s_ffn = feed_forward_network(512, 2048)\n",
    "print(s_ffn(tf.random.uniform((64, 50, 512))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranformer is consist of Encoder and Decoder\n",
    "#for Encoder, it is consist of many encoder layers, for Decoder, it is consist of many decoder layers.\n",
    "#each encoder layer have self attention layer, which is multi-head self attention layer, and feed forward network. And between them, it still has \n",
    "#normalization layer\n",
    "#and for each decoder layer, after self attention layer , it has a attention layer, which will deal with the result from encoder output and weights\n",
    "#after it, is a feed forward network. It still has normalization layer.\n",
    "#Following, will create multi-head attention layer, then encoder layer, decoder layer, then Encoder, Decoder, Transformer.\n",
    "\n",
    "#1. Multi-head Layer\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_head, dmodel):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.dmodel = dmodel\n",
    "        assert dmodel % num_head == 0\n",
    "        \n",
    "        self.depth = dmodel // num_head\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(dmodel)\n",
    "        self.wk = tf.keras.layers.Dense(dmodel)\n",
    "        self.wv = tf.keras.layers.Dense(dmodel)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(dmodel)\n",
    "        \n",
    "    def split_head(self, x, batch_size):\n",
    "        \"\"\"split last dimention into (num_head, depth)\n",
    "            then tranpose the shape into (batch_size, num_head, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_head, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, q, k, v):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wk(v)\n",
    "        \n",
    "        q = self.split_head(q, batch_size)\n",
    "        k = self.split_head(k, batch_size)\n",
    "        v = self.split_head(v, batch_size)\n",
    "        \n",
    "        attention, weights = scaled_dot_product_attention(q, k, v)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) #make the dimention into (batch_size, seq_len, num_head, depth)\n",
    "        #now concate \n",
    "        concate_output = tf.reshape(attention, (batch_size, -1, self.dmodel))\n",
    "        # again do the linear change\n",
    "        output = self.dense(concate_output)\n",
    "        \n",
    "        return output, weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"consist of one multi head attention layer, normalization layer, feed forward network, normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_head, dmodel, dff, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "    \n",
    "        self.multi_head = MultiHeadAttentionLayer(num_head, dmodel)\n",
    "        self.forward_network = feed_forward_network(dmodel, dff)\n",
    "        \n",
    "        self.normalization1 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\n",
    "        self.normalization2 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\n",
    "        \n",
    "        #alseo we can add 2 dropout layer\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, is_training):\n",
    "        attend_out, attention_weights = self.multi_head(x, x, x)\n",
    "        attend_out = self.dropout1(attend_out, training = is_training)\n",
    "        out1 = self.normalization1(x+attend_out)\n",
    "        out_ffn = self.forward_network(out1)\n",
    "        out_ffn = self.dropout2(out_ffn, training = is_training)\n",
    "        out2 = self.normalization2(out1 + out_ffn)\n",
    "        \n",
    "        return out2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder out_ffn shape: (64, 43, 512)\n",
      "encoder out_ffn shape after norm: (64, 43, 512)\n",
      "(64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "#test Encode layer\n",
    "test_enc_layer = EncoderLayer(8, 512, 2048)\n",
    "\n",
    "test_enc_layer_output = test_enc_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False)\n",
    "\n",
    "print(test_enc_layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"1. multi head attention layer, dropout, normalization layer\n",
    "        2. multi head attention layer, dropout, normalization layer, in this layer, used the output of the encoder layer\n",
    "        3. feed forward network layer, dropout, normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_head, dmodel, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head1= MultiHeadAttentionLayer(num_head, dmodel)\n",
    "        self.multi_head2= MultiHeadAttentionLayer(num_head, dmodel)\n",
    "        \n",
    "        self.forward_network = feed_forward_network(dmodel, dff)\n",
    "        \n",
    "        self.normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.normalization3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, encode_output,  is_training):\n",
    "        input_attention, input_attention_weights = self.multi_head1(x, x, x)\n",
    "        input_attention = self.dropout1(input_attention, training = is_training)\n",
    "        input_out1 = self.normalization1(x+input_attention)\n",
    "        \n",
    "        attention_2, attention_weights_2 = self.multi_head2(input_out1, encode_output, encode_output)\n",
    "        attention_2 = self.dropout2(attention_2, training = is_training)\n",
    "        out2 = self.normalization2(input_out1 + attention_2)\n",
    "        \n",
    "        out_ffn = self.forward_network(out2)\n",
    "        out_ffn = self.dropout3(out_ffn, training = is_training)\n",
    "        out3 = self.normalization3(out2 + out_ffn)\n",
    "        \n",
    "        return out3, input_attention_weights, attention_weights_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "#test for decoder layer\n",
    "test_dec_layer = DecoderLayer(8, 512, 2048)\n",
    "test_dec_out,_,_ = test_dec_layer(tf.random.uniform((64, 4, 512)), test_enc_layer_output, False)\n",
    "print(test_dec_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"1.embedding input \n",
    "        2. add position encoding\n",
    "        3. multi encoder layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, num_head, dmodel, dff, input_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dmodel = dmodel\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, dmodel)\n",
    "        self.pos_enc = get_position_embedding(max_position_encoding, dmodel)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(num_head, dmodel, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        #embedding, pos_encode\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.dmodel, dtype = tf.float32))\n",
    "        x += self.pos_enc[:,:seq_len,:]\n",
    "        \n",
    "        x = self.dropout(x, training= training)\n",
    "        #now encoder\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder out_ffn shape: (64, 62, 512)\n",
      "encoder out_ffn shape after norm: (64, 62, 512)\n",
      "encoder out_ffn shape: (64, 62, 512)\n",
      "encoder out_ffn shape after norm: (64, 62, 512)\n",
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "#test for encoder\n",
    "test_encoder = Encoder(num_layers=2,num_head=8,dmodel=512, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         max_position_encoding=10000)\n",
    "\n",
    "test_encoder_output = test_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False)\n",
    "\n",
    "print (test_encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"1. embedding + pos_encode\n",
    "        2.multi decode layers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, num_head, dmodel, dff, output_vocab_size, max_pos_encode, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dmodel = dmodel\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(output_vocab_size, dmodel)\n",
    "        self.pos_encode = get_position_embedding(max_pos_encode, dmodel)\n",
    "        \n",
    "        self.decode_layers = [DecoderLayer(num_head, dmodel, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, encode_output, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.dmodel, tf.float32))\n",
    "        x += self.pos_encode[:,:seq_len, :]\n",
    "        #dropout x\n",
    "        x = self.dropout(x, training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, weights1, weights2 = self.decode_layers[i](x, encode_output, training)\n",
    "            attention_weights['layer{}_weights1'.format(i+1)] = weights1\n",
    "            attention_weights['layer{}_weights2'.format(i+1)] = weights2\n",
    "        \n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "#test for decoder\n",
    "test_decoder = Decoder(num_layers=2,num_head=8,dmodel=512, \n",
    "                         dff=2048, output_vocab_size=8000,\n",
    "                         max_pos_encode=6000)\n",
    "\n",
    "test_decoder_output, _ = test_decoder(tf.random.uniform((64, 4)), test_encoder_output,\n",
    "                                       training=False)\n",
    "\n",
    "print (test_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"1. encoder\n",
    "        2. decoder\n",
    "        3. linear output\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, num_head, dmodel, dff, input_vocab_size, output_vocab_size, input_max_pos, output_max_pos, drate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, num_head, dmodel, dff, input_vocab_size, input_max_pos, drate)\n",
    "        self.decoder = Decoder(num_layers, num_head, dmodel, dff, output_vocab_size, output_max_pos, drate)\n",
    "        self.linear_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        \n",
    "    def call(self, enc_x, dec_x, training):\n",
    "        encoder_output = self.encoder(enc_x, training)\n",
    "        dec_output, attention_weights = self.decoder(dec_x, encoder_output, training)\n",
    "        output = self.linear_layer(dec_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4, 8000)\n"
     ]
    }
   ],
   "source": [
    "#test_transformer\n",
    "test_trans = Transformer(2, 8, 512, 2048, input_vocab_size=8500, output_vocab_size=8000, input_max_pos=10000, output_max_pos=6000)\n",
    "out, atte = test_trans(tf.random.uniform((64, 62)), tf.random.uniform((64, 4)), False)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the decoder only deal with the words, we need decoder to deal with the picture feature and also add attention to picture when it generate words\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.units = units\n",
    "        self.enc = tf.keras.layers.Dense(units)\n",
    "        self.hw = tf.keras.layers.Dense(units)\n",
    "        self.score = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, feature, hidden):\n",
    "        #hidden shape is (batchsize, units)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1) # (batchsize, 1, units)\n",
    "        print('hidden_with_time_axis:', hidden_with_time_axis.shape)\n",
    "        score = tf.math.tanh(self.enc(feature) + self.hw(hidden_with_time_axis))\n",
    "        print('score:', score.shape)\n",
    "        attention_weights = tf.keras.backend.softmax(self.score(score), axis = 1)\n",
    "        print('attention_weights:', attention_weights.shape)\n",
    "        context_tensor = attention_weights * feature\n",
    "        print('context_tensor bef:', context_tensor.shape)\n",
    "        #context_tensor = tf.math.reduce_sum(context_tensor, axis = 1, keepdims=True)\n",
    "        print('context_tensor aft:', context_tensor.shape)\n",
    "        return context_tensor, attention_weights\n",
    "\n",
    "class DecoderWithImageLayer(DecoderLayer):\n",
    "    def __init__(self, num_head, dmodel, dff, units, drate=0.1):\n",
    "        super(DecoderWithImageLayer, self).__init__(num_head, dmodel, dff)\n",
    "        self.units = units\n",
    "        self.img_dens = tf.keras.layers.Dense(dmodel)\n",
    "        self.img_attention = BahdanauAttention(units)\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform')\n",
    "        self.img_dens2 = tf.keras.layers.Dense(units)\n",
    "        self.droput_img = tf.keras.layers.Dropout(drate)\n",
    "        \n",
    "        \n",
    "    def call(self, x, enc_output, feature, hidden, training):\n",
    "        #input embedding and pos_encode\n",
    "        x_attention, weights = self.multi_head1(x,x,x)\n",
    "        x_attention = self.dropout1(x_attention, training = training)\n",
    "        x_out = self.normalization1(x + x_attention)\n",
    "        #img \n",
    "        print('feature shape before:', feature.shape)\n",
    "        features = self.img_dens(feature)\n",
    "        features = tf.nn.relu(features)\n",
    "        print('feature shape:', features.shape)\n",
    "        #decode input and encode output attention \n",
    "        x_enc_attention, x_enc_weights = self.multi_head2(x_out,enc_output,enc_output)\n",
    "        x_enc_attention = self.dropout2(x_enc_attention, training = training)\n",
    "        x_enc_out = self.normalization2(x_out + x_enc_attention)\n",
    "        #image feature attention and decode input\n",
    "        img_context, img_attention = self.img_attention(features, hidden)\n",
    "        print('img_context shape before:', img_context.shape)\n",
    "        print('x shape', x.shape)\n",
    "        x_img_concate = tf.concat([img_context, x], axis = -1)\n",
    "        print('x_img_concate_shape:', x_img_concate.shape)\n",
    "        x_img_out, state = self.gru(x_img_concate)\n",
    "        print('x_img_gru_shape:', x_img_out.shape)\n",
    "        x_img_out = self.img_dens2(x_img_out)\n",
    "        print('x_img_out_shape:', x_img_out.shape)\n",
    "        x_img_out = self.droput_img(x_img_out, training)\n",
    "        #x_img_out = self.img_nor(x_img_concate + x_img_out)\n",
    "        #concate x_enc_out and x_img_out\n",
    "        concate_all = tf.concat([x_enc_out, x_img_out], axis = -1)\n",
    "        \n",
    "        out_ffn = self.forward_network(concate_all)\n",
    "        out_ffn = self.dropout3(out_ffn, training = training)\n",
    "        out3 = self.normalization3(x_enc_out + out_ffn)\n",
    "        \n",
    "        return out3, weights, x_enc_weights, state, img_attention\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4096)\n",
      "(1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "#test for attention\n",
    "test_at = BahdanauAttention(512)\n",
    "test_con, test_atten = test_at(tf.random.uniform((1,4096)), tf.zeros((1, 512)))\n",
    "print(test_con.shape)\n",
    "print(test_atten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape before: (64, 4, 1024)\n",
      "feature shape: (64, 4, 512)\n",
      "hidden_with_time_axis: (64, 1, 512)\n",
      "score: (64, 4, 512)\n",
      "attention_weights: (64, 4, 1)\n",
      "context_tensor bef: (64, 4, 512)\n",
      "context_tensor aft: (64, 4, 512)\n",
      "img_context shape before: (64, 4, 512)\n",
      "x shape (64, 4, 512)\n",
      "x_img_concate_shape: (64, 4, 1024)\n",
      "x_img_gru_shape: (64, 4, 512)\n",
      "x_img_out_shape: (64, 4, 512)\n",
      "(64, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "#test for decode image\n",
    "test_dec_img = DecoderWithImageLayer(8, 512, 2048, 512)\n",
    "test_d_out,_,_,_,_ = test_dec_img(tf.random.uniform((64, 4, 512)), test_enc_layer_output, tf.random.uniform((64,4,1024)), tf.zeros((64, 512)), False)\n",
    "print(test_d_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithImage(tf.keras.layers.Layer):\n",
    "    \"\"\"1. embedding + pos_encode\n",
    "        2.multi decode layers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, num_head, dmodel, dff, output_vocab_size, max_pos_encode, units, dropout_rate=0.1):\n",
    "        super(DecoderWithImage, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dmodel = dmodel\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(output_vocab_size, dmodel)\n",
    "        self.pos_encode = get_position_embedding(max_pos_encode, dmodel)\n",
    "        \n",
    "        self.decode_layers = [DecoderWithImageLayer(num_head, dmodel, dff, units, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, encode_output, feature, hidden, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.dmodel, tf.float32))\n",
    "        x += self.pos_encode[:,:seq_len, :]\n",
    "        #dropout x\n",
    "        x = self.dropout(x, training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            print('in decoder : x shape', x.shape, ' layer:', i)\n",
    "            x, weights1, weights2, hidden, img_weights = self.decode_layers[i](x, encode_output, feature, hidden, training)\n",
    "            print('in decoder: after layer, x shape:', x.shape, 'decoder layer :', i)\n",
    "            attention_weights['layer{}_weights1'.format(i+1)] = weights1\n",
    "            attention_weights['layer{}_weights2'.format(i+1)] = weights2\n",
    "            attention_weights['layer{}_img_weights'.format(i+1)] = img_weights\n",
    "        \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decoder : x shape (64, 4, 512)  layer: 0\n",
      "feature shape before: (64, 4, 1024)\n",
      "feature shape: (64, 4, 512)\n",
      "hidden_with_time_axis: (64, 1, 512)\n",
      "score: (64, 4, 512)\n",
      "attention_weights: (64, 4, 1)\n",
      "context_tensor bef: (64, 4, 512)\n",
      "context_tensor aft: (64, 4, 512)\n",
      "img_context shape before: (64, 4, 512)\n",
      "x shape (64, 4, 512)\n",
      "x_img_concate_shape: (64, 4, 1024)\n",
      "x_img_gru_shape: (64, 4, 512)\n",
      "x_img_out_shape: (64, 4, 512)\n",
      "in decoder: after layer, x shape: (64, 4, 512) decoder layer : 0\n",
      "in decoder : x shape (64, 4, 512)  layer: 1\n",
      "feature shape before: (64, 4, 1024)\n",
      "feature shape: (64, 4, 512)\n",
      "hidden_with_time_axis: (64, 1, 512)\n",
      "score: (64, 4, 512)\n",
      "attention_weights: (64, 4, 1)\n",
      "context_tensor bef: (64, 4, 512)\n",
      "context_tensor aft: (64, 4, 512)\n",
      "img_context shape before: (64, 4, 512)\n",
      "x shape (64, 4, 512)\n",
      "x_img_concate_shape: (64, 4, 1024)\n",
      "x_img_gru_shape: (64, 4, 512)\n",
      "x_img_out_shape: (64, 4, 512)\n",
      "in decoder: after layer, x shape: (64, 4, 512) decoder layer : 1\n",
      "(64, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "#test for decoder image \n",
    "test_dec_img = DecoderWithImage(2, 8, 512, 2048, 8000, 6000, 512)\n",
    "test_dec_img_out,_ = test_dec_img(tf.random.uniform((64,4)), tf.random.uniform((64,1,512)), tf.random.uniform((64,4, 1024)), tf.zeros((64, 512)), False)\n",
    "print(test_dec_img_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "class TransformerWithImage(tf.keras.Model):\n",
    "    \"\"\"1. encoder\n",
    "        2. decoder\n",
    "        3. linear output\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, num_head, dmodel, dff, input_vocab_size, output_vocab_size, input_max_pos, output_max_pos, units, drate=0.1):\n",
    "        super(TransformerWithImage, self).__init__()\n",
    "        self.units = units\n",
    "        self.encoder = Encoder(num_layers, num_head, dmodel, dff, input_vocab_size, input_max_pos, drate)\n",
    "        self.decoder = DecoderWithImage(num_layers, num_head, dmodel, dff, output_vocab_size, output_max_pos, units, drate)\n",
    "        self.linear_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        \n",
    "    def call(self, enc_x, dec_x, feature, hidden, training):\n",
    "        encoder_output = self.encoder(enc_x, training)\n",
    "        dec_output, attention_weights = self.decoder(dec_x, encoder_output, feature, hidden, training)\n",
    "        output = self.linear_layer(dec_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decoder : x shape (64, 4, 512)  layer: 0\n",
      "feature shape before: (64, 4, 1024)\n",
      "feature shape: (64, 4, 512)\n",
      "hidden_with_time_axis: (64, 1, 512)\n",
      "score: (64, 4, 512)\n",
      "attention_weights: (64, 4, 1)\n",
      "context_tensor bef: (64, 4, 512)\n",
      "context_tensor aft: (64, 4, 512)\n",
      "img_context shape before: (64, 4, 512)\n",
      "x shape (64, 4, 512)\n",
      "x_img_concate_shape: (64, 4, 1024)\n",
      "x_img_gru_shape: (64, 4, 512)\n",
      "x_img_out_shape: (64, 4, 512)\n",
      "in decoder: after layer, x shape: (64, 4, 512) decoder layer : 0\n",
      "in decoder : x shape (64, 4, 512)  layer: 1\n",
      "feature shape before: (64, 4, 1024)\n",
      "feature shape: (64, 4, 512)\n",
      "hidden_with_time_axis: (64, 1, 512)\n",
      "score: (64, 4, 512)\n",
      "attention_weights: (64, 4, 1)\n",
      "context_tensor bef: (64, 4, 512)\n",
      "context_tensor aft: (64, 4, 512)\n",
      "img_context shape before: (64, 4, 512)\n",
      "x shape (64, 4, 512)\n",
      "x_img_concate_shape: (64, 4, 1024)\n",
      "x_img_gru_shape: (64, 4, 512)\n",
      "x_img_out_shape: (64, 4, 512)\n",
      "in decoder: after layer, x shape: (64, 4, 512) decoder layer : 1\n",
      "(64, 4, 8000)\n"
     ]
    }
   ],
   "source": [
    "#test for Transformer image\n",
    "test_tran_img = TransformerWithImage(2, 8, 512, 2048, 8500, 8000, 10000, 6000, 512)\n",
    "test_tran_out,_ = test_tran_img(tf.random.uniform((64,24)), tf.random.uniform((64,4)), tf.random.uniform((64,4,1024)), tf.zeros((64,512)), False)\n",
    "print(test_tran_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "#Need customize learning rate schedule, the formula is lrate = rsqrt(dmodel) * min(rsqrt(step), step * warmup_step**-1.5)\n",
    "\n",
    "class CustomizeLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, dmodel, warmup_step = 4000):\n",
    "        self.dmodel = tf.cast(dmodel, dtype=tf.float32)\n",
    "        self.warmup_step = warmup_step\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        a = tf.math.rsqrt(self.dmodel)\n",
    "        b = tf.math.rsqrt(step)\n",
    "        c = step * (self.warmup_step ** -1.5)\n",
    "        return a * tf.math.minimum(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取图片的features\n",
    "#使用标准coco的图片，从https://cs.stanford.edu/people/karpathy/deepimagesent/下可以下载到这些图片的features\n",
    "#这样可以不用训练，直接拿到图片的feature\n",
    "import scipy.io as io\n",
    "#加载训练好的图片的feature到feature_struct中\n",
    "#生成图片id与feature列标的对应数据 img_map\n",
    "def generate_coco_image_feature(img_feats_file, img_ids_feats):\n",
    "    feature_struct = io.loadmat(img_feats_file)\n",
    "    img_vgg_features = feature_struct['feats']\n",
    "    img_ids_feats_col = open(img_ids_feats).read().splitlines()\n",
    "    img_map = {}\n",
    "    for ids in img_ids_feats_col:\n",
    "        ids_split = ids.split()\n",
    "        img_map[ids_split[0]] = int(ids_split[1])\n",
    "    return img_vgg_features, img_map\n",
    "\n",
    "#根据图片id,取得相应的特征数据\n",
    "def get_image_matrix(feature_struct, img_map, image_ids):\n",
    "    rows = len(image_ids)\n",
    "    img_matrix = np.zeros((rows, feature_struct.shape[0]))\n",
    "    for i in range(rows):\n",
    "        img_matrix[i,:] = feature_struct[:,img_map[image_ids[i]]]\n",
    "    return img_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "#prepare data, I want to use the vqa train data \n",
    "questions = open(\"./questions_train2014.txt\", \"r\").read().splitlines()\n",
    "img_ids = open(\"./images_train2014.txt\", \"r\").read().splitlines()\n",
    "answers = open(\"./answers_train2014_modal.txt\",\"r\").read().splitlines()\n",
    "#prepare picture feature\n",
    "img_feature_struct, img_id_feature_map = generate_coco_image_feature(\"./vgg_feats.mat\", \"./coco_vgg_IDMap.txt\")\n",
    "\n",
    "index = (int)(len(questions) * 0.8)\n",
    "train_questions=questions[0:index]\n",
    "train_img_ids = img_ids[0:index]\n",
    "train_answers = answers[0:index]\n",
    "\n",
    "test_questions = questions[index:]\n",
    "test_img_ids = img_ids[index:]\n",
    "test_answers = answers[index:]\n",
    "\n",
    "#prepare training data\n",
    "from collections import defaultdict\n",
    "\n",
    "#1. find out most frequence answers\n",
    "max_ans = 1000\n",
    "answers_all = defaultdict(int)\n",
    "for ans in train_answers:\n",
    "    answers_all[ans] += 1\n",
    "answers_all = sorted(answers_all.items(), key=operator.itemgetter(1), reverse= True)[0:max_ans]\n",
    "top_ans, top_freq = zip(*answers_all)\n",
    "q_new , img_new, ans_new = [], [], []\n",
    "\n",
    "#2.get training question, image, answers\n",
    "for q, img, ans in zip(train_questions, train_img_ids, train_answers):\n",
    "    if ans in top_ans:\n",
    "        q_new.append(q)\n",
    "        img_new.append(img)\n",
    "        ans_new.append(ans)\n",
    "\n",
    "train_questions_mlp  = q_new\n",
    "train_img_ids_mlp = img_new\n",
    "train_answers_mlp = ans_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get input_vocab_size\n",
    "def tokenize(text):\n",
    "    txt_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    txt_tokenizer.fit_on_texts(text)\n",
    "    tensor = txt_tokenizer.texts_to_sequences(text)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, txt_tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu_tensor, qu_token = tokenize(train_questions_mlp)\n",
    "an_tensor, an_token = tokenize(train_answers_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf.data\n",
    "BUFFER_SIZE = len(qu_tensor)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(qu_tensor)//BATCH_SIZE\n",
    "\n",
    "input_vocab_size = len(qu_token.word_index) + 1\n",
    "output_vocab_size = len(an_token.word_index) + 1\n",
    "\n",
    "def img_features(qu, img, an):\n",
    "    img = str(img, encoding = \"utf-8\")\n",
    "    img_matrix = np.array(img_feature_struct[:,img_id_feature_map[img]])\n",
    "    img_matrix = img_matrix.reshape(an.shape[-1], -1)\n",
    "    return qu, img_matrix, an\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((qu_tensor, train_img_ids_mlp, an_tensor))\n",
    "dataset = dataset.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "          img_features, [item1, item2, item3], [tf.int32, tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2    1 7623 ...    0    0    0]\n",
      " [   9   10  524 ...    0    0    0]\n",
      " [   2    5   14 ...    0    0    0]\n",
      " ...\n",
      " [   3 1154    1 ...    0    0    0]\n",
      " [   2    5    7 ...    0    0    0]\n",
      " [   2    5    7 ...    0    0    0]], shape=(64, 22), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-0.         -0.         -0.         ... -0.         -0.\n",
      "    0.2905249 ]\n",
      "  [-0.         -0.         -0.         ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [-0.          2.72104    -0.         ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [-0.         -0.          0.76282835 ... -0.          0.19101006\n",
      "    4.366334  ]]\n",
      "\n",
      " [[-0.         -0.          0.43651015 ... -0.         -0.\n",
      "    2.3054607 ]\n",
      "  [-0.         -0.          0.10234973 ...  7.030856   -0.\n",
      "   -0.        ]\n",
      "  [-0.          3.4200547  -0.         ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [-0.         -0.         -0.         ... -0.         -0.\n",
      "    3.4806335 ]]\n",
      "\n",
      " [[ 0.8935639  -0.          3.895008   ... -0.          4.7719116\n",
      "   -0.        ]\n",
      "  [-0.          0.28672308 -0.         ... -0.          0.79471344\n",
      "   -0.        ]\n",
      "  [-0.         -0.         -0.         ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [-0.          0.12181389  0.5757663  ... -0.          3.0587935\n",
      "    1.8412797 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.1541286  -0.          2.94812    ... -0.          2.583304\n",
      "   -0.        ]\n",
      "  [ 0.72206086  0.45999232 -0.         ... -0.          1.9921151\n",
      "   -0.        ]\n",
      "  [-0.         -0.          1.168895   ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [ 1.5604911   0.853795   -0.         ... -0.          2.1056812\n",
      "    0.6152961 ]]\n",
      "\n",
      " [[-0.          4.979607   -0.         ...  0.22044379  3.5817912\n",
      "   -0.        ]\n",
      "  [-0.         -0.         -0.         ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [-0.         -0.         -0.         ... -0.         -0.\n",
      "    1.1503391 ]\n",
      "  [-0.         -0.          2.9380198  ... -0.          0.95326513\n",
      "   -0.        ]]\n",
      "\n",
      " [[ 3.305181   -0.          1.883952   ... -0.         -0.\n",
      "   -0.        ]\n",
      "  [-0.         -0.          1.404874   ... -0.          3.3914628\n",
      "   -0.        ]\n",
      "  [-0.          1.1198051   2.3339365  ... -0.         -0.\n",
      "    1.5002666 ]\n",
      "  [-0.          3.5651987  -0.         ... -0.         -0.\n",
      "   -0.        ]]], shape=(64, 4, 1024), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[  1   0   0   0]\n",
      " [  6   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [ 12   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [ 24   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [410   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [884   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [476  23   0   0]\n",
      " [  2   0   0   0]\n",
      " [  3   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [573   0   0   0]\n",
      " [190   0   0   0]\n",
      " [  3   0   0   0]\n",
      " [ 82  77   0   0]\n",
      " [  7  13   4   0]\n",
      " [  1   0   0   0]\n",
      " [ 44   0   0   0]\n",
      " [  3   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [745   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [ 11   0   0   0]\n",
      " [ 12   0   0   0]\n",
      " [ 11   0   0   0]\n",
      " [  6   0   0   0]\n",
      " [ 56   0   0   0]\n",
      " [149   0   0   0]\n",
      " [ 11   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  6   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [111   0   0   0]\n",
      " [ 59   0   0   0]\n",
      " [ 36   0   0   0]\n",
      " [603   0   0   0]\n",
      " [185   0   0   0]\n",
      " [ 21   0   0   0]\n",
      " [855   0   0   0]\n",
      " [  2   0   0   0]\n",
      " [ 16   0   0   0]\n",
      " [187   0   0   0]\n",
      " [ 30   0   0   0]\n",
      " [422   0   0   0]\n",
      " [ 52   0   0   0]\n",
      " [  8   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [ 41   0   0   0]\n",
      " [  1   0   0   0]\n",
      " [  2   0   0   0]], shape=(64, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "example_qu_batch, example_img_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "print(example_qu_batch)\n",
    "print(example_img_batch)\n",
    "print(example_target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "transformer = TransformerWithImage(num_layers = 2, num_head = 8, dmodel = d_model, dff = 2048, \\\n",
    "                                   input_vocab_size= input_vocab_size, output_vocab_size = output_vocab_size,\\\n",
    "                                  input_max_pos = 10000, output_max_pos=6000, units=d_model)\n",
    "\n",
    "learning_rate = CustomizeLearningRateSchedule(dmodel=d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(real, pred):\n",
    "    loss_ob = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_ob(real, pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(ques, img_tensor, target):\n",
    "    # initializing the hidden state for each batch\n",
    "    print('img tensor shape', img_tensor.shape)\n",
    "    print('qu shape:{}, img_shape:{}, tar_shape:{}'.format(ques.shape, img_tensor.shape, target.shape))\n",
    "    hidden = tf.zeros((BATCH_SIZE,d_model))\n",
    "    dec_input = tf.zeros(target.shape)\n",
    "    print('')\n",
    "    with tf.GradientTape() as tape:\n",
    "        # passing the questions, reply start, image features to the transformer\n",
    "        predictions, _ = transformer(ques,dec_input, img_tensor, hidden, True)\n",
    "        print('predictions:', predictions)\n",
    "        loss = loss_fun(target, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.4938 Accuracy 0.8711\n",
      "Epoch 1 Batch 50 Loss 0.4986 Accuracy 0.8638\n",
      "Epoch 1 Batch 100 Loss 0.5043 Accuracy 0.8630\n",
      "Epoch 1 Batch 150 Loss 0.5172 Accuracy 0.8588\n",
      "Epoch 1 Batch 200 Loss 0.5181 Accuracy 0.8588\n",
      "Epoch 1 Batch 250 Loss 0.5192 Accuracy 0.8587\n",
      "Epoch 1 Batch 300 Loss 0.5214 Accuracy 0.8583\n",
      "Epoch 1 Batch 350 Loss 0.5225 Accuracy 0.8585\n",
      "Epoch 1 Batch 400 Loss 0.5241 Accuracy 0.8584\n",
      "Epoch 1 Batch 450 Loss 0.5233 Accuracy 0.8582\n",
      "Epoch 1 Batch 500 Loss 0.5224 Accuracy 0.8583\n",
      "Epoch 1 Batch 550 Loss 0.5222 Accuracy 0.8583\n",
      "Epoch 1 Batch 600 Loss 0.5231 Accuracy 0.8580\n",
      "Epoch 1 Batch 650 Loss 0.5241 Accuracy 0.8577\n",
      "Epoch 1 Batch 700 Loss 0.5258 Accuracy 0.8574\n",
      "Epoch 1 Batch 750 Loss 0.5267 Accuracy 0.8572\n",
      "Epoch 1 Batch 800 Loss 0.5275 Accuracy 0.8572\n",
      "Epoch 1 Batch 850 Loss 0.5283 Accuracy 0.8570\n",
      "Epoch 1 Batch 900 Loss 0.5295 Accuracy 0.8565\n",
      "Epoch 1 Batch 950 Loss 0.5294 Accuracy 0.8568\n",
      "Epoch 1 Batch 1000 Loss 0.5296 Accuracy 0.8568\n",
      "Epoch 1 Batch 1050 Loss 0.5293 Accuracy 0.8568\n",
      "Epoch 1 Batch 1100 Loss 0.5294 Accuracy 0.8566\n",
      "Epoch 1 Batch 1150 Loss 0.5304 Accuracy 0.8565\n",
      "Epoch 1 Batch 1200 Loss 0.5311 Accuracy 0.8564\n",
      "Epoch 1 Batch 1250 Loss 0.5313 Accuracy 0.8564\n",
      "Epoch 1 Batch 1300 Loss 0.5317 Accuracy 0.8563\n",
      "Epoch 1 Batch 1350 Loss 0.5318 Accuracy 0.8563\n",
      "Epoch 1 Batch 1400 Loss 0.5314 Accuracy 0.8565\n",
      "Epoch 1 Batch 1450 Loss 0.5318 Accuracy 0.8564\n",
      "Epoch 1 Batch 1500 Loss 0.5315 Accuracy 0.8563\n",
      "Epoch 1 Batch 1550 Loss 0.5318 Accuracy 0.8564\n",
      "Epoch 1 Batch 1600 Loss 0.5321 Accuracy 0.8563\n",
      "Epoch 1 Batch 1650 Loss 0.5328 Accuracy 0.8563\n",
      "Epoch 1 Batch 1700 Loss 0.5329 Accuracy 0.8563\n",
      "Epoch 1 Batch 1750 Loss 0.5329 Accuracy 0.8563\n",
      "Epoch 1 Batch 1800 Loss 0.5329 Accuracy 0.8564\n",
      "Epoch 1 Batch 1850 Loss 0.5333 Accuracy 0.8562\n",
      "Epoch 1 Batch 1900 Loss 0.5331 Accuracy 0.8562\n",
      "Epoch 1 Batch 1950 Loss 0.5331 Accuracy 0.8561\n",
      "Epoch 1 Batch 2000 Loss 0.5329 Accuracy 0.8561\n",
      "Epoch 1 Batch 2050 Loss 0.5327 Accuracy 0.8562\n",
      "Epoch 1 Batch 2100 Loss 0.5322 Accuracy 0.8563\n",
      "Epoch 1 Batch 2150 Loss 0.5321 Accuracy 0.8562\n",
      "Epoch 1 Batch 2200 Loss 0.5320 Accuracy 0.8562\n",
      "Epoch 1 Batch 2250 Loss 0.5318 Accuracy 0.8561\n",
      "Epoch 1 Batch 2300 Loss 0.5310 Accuracy 0.8563\n",
      "Epoch 1 Batch 2350 Loss 0.5309 Accuracy 0.8562\n",
      "Epoch 1 Batch 2400 Loss 0.5306 Accuracy 0.8563\n",
      "Epoch 1 Batch 2450 Loss 0.5301 Accuracy 0.8564\n",
      "Epoch 1 Batch 2500 Loss 0.5298 Accuracy 0.8565\n",
      "Epoch 1 Batch 2550 Loss 0.5295 Accuracy 0.8566\n",
      "Epoch 1 Batch 2600 Loss 0.5291 Accuracy 0.8567\n",
      "Epoch 1 Batch 2650 Loss 0.5286 Accuracy 0.8567\n",
      "Epoch 1 Loss 0.5282 Accuracy 0.8567\n",
      "Time taken for 1 epoch: 3766.180097103119 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.4075 Accuracy 0.8672\n",
      "Epoch 2 Batch 50 Loss 0.4815 Accuracy 0.8646\n",
      "Epoch 2 Batch 100 Loss 0.4804 Accuracy 0.8646\n",
      "Epoch 2 Batch 150 Loss 0.4760 Accuracy 0.8656\n",
      "Epoch 2 Batch 200 Loss 0.4758 Accuracy 0.8666\n",
      "Epoch 2 Batch 250 Loss 0.4768 Accuracy 0.8664\n",
      "Epoch 2 Batch 300 Loss 0.4770 Accuracy 0.8656\n",
      "Epoch 2 Batch 350 Loss 0.4769 Accuracy 0.8657\n",
      "Epoch 2 Batch 400 Loss 0.4775 Accuracy 0.8660\n",
      "Epoch 2 Batch 450 Loss 0.4783 Accuracy 0.8660\n",
      "Epoch 2 Batch 500 Loss 0.4790 Accuracy 0.8660\n",
      "Epoch 2 Batch 550 Loss 0.4777 Accuracy 0.8665\n",
      "Epoch 2 Batch 600 Loss 0.4777 Accuracy 0.8664\n",
      "Epoch 2 Batch 650 Loss 0.4784 Accuracy 0.8664\n",
      "Epoch 2 Batch 700 Loss 0.4773 Accuracy 0.8665\n",
      "Epoch 2 Batch 750 Loss 0.4769 Accuracy 0.8666\n",
      "Epoch 2 Batch 800 Loss 0.4772 Accuracy 0.8666\n",
      "Epoch 2 Batch 850 Loss 0.4764 Accuracy 0.8667\n",
      "Epoch 2 Batch 900 Loss 0.4765 Accuracy 0.8665\n",
      "Epoch 2 Batch 950 Loss 0.4771 Accuracy 0.8662\n",
      "Epoch 2 Batch 1000 Loss 0.4773 Accuracy 0.8662\n",
      "Epoch 2 Batch 1050 Loss 0.4766 Accuracy 0.8664\n",
      "Epoch 2 Batch 1100 Loss 0.4768 Accuracy 0.8665\n",
      "Epoch 2 Batch 1150 Loss 0.4765 Accuracy 0.8667\n",
      "Epoch 2 Batch 1200 Loss 0.4768 Accuracy 0.8665\n",
      "Epoch 2 Batch 1250 Loss 0.4763 Accuracy 0.8666\n",
      "Epoch 2 Batch 1300 Loss 0.4768 Accuracy 0.8665\n",
      "Epoch 2 Batch 1350 Loss 0.4769 Accuracy 0.8664\n",
      "Epoch 2 Batch 1400 Loss 0.4768 Accuracy 0.8663\n",
      "Epoch 2 Batch 1450 Loss 0.4772 Accuracy 0.8664\n",
      "Epoch 2 Batch 1500 Loss 0.4774 Accuracy 0.8663\n",
      "Epoch 2 Batch 1550 Loss 0.4773 Accuracy 0.8663\n",
      "Epoch 2 Batch 1600 Loss 0.4771 Accuracy 0.8664\n",
      "Epoch 2 Batch 1650 Loss 0.4774 Accuracy 0.8664\n",
      "Epoch 2 Batch 1700 Loss 0.4779 Accuracy 0.8664\n",
      "Epoch 2 Batch 1750 Loss 0.4779 Accuracy 0.8664\n",
      "Epoch 2 Batch 1800 Loss 0.4779 Accuracy 0.8664\n",
      "Epoch 2 Batch 1850 Loss 0.4775 Accuracy 0.8664\n",
      "Epoch 2 Batch 1900 Loss 0.4770 Accuracy 0.8665\n",
      "Epoch 2 Batch 1950 Loss 0.4769 Accuracy 0.8665\n",
      "Epoch 2 Batch 2000 Loss 0.4769 Accuracy 0.8664\n",
      "Epoch 2 Batch 2050 Loss 0.4776 Accuracy 0.8663\n",
      "Epoch 2 Batch 2100 Loss 0.4781 Accuracy 0.8663\n",
      "Epoch 2 Batch 2150 Loss 0.4785 Accuracy 0.8662\n",
      "Epoch 2 Batch 2200 Loss 0.4786 Accuracy 0.8662\n",
      "Epoch 2 Batch 2250 Loss 0.4783 Accuracy 0.8664\n",
      "Epoch 2 Batch 2300 Loss 0.4785 Accuracy 0.8663\n",
      "Epoch 2 Batch 2350 Loss 0.4787 Accuracy 0.8663\n",
      "Epoch 2 Batch 2400 Loss 0.4784 Accuracy 0.8664\n",
      "Epoch 2 Batch 2450 Loss 0.4785 Accuracy 0.8665\n",
      "Epoch 2 Batch 2500 Loss 0.4782 Accuracy 0.8665\n",
      "Epoch 2 Batch 2550 Loss 0.4781 Accuracy 0.8665\n",
      "Epoch 2 Batch 2600 Loss 0.4781 Accuracy 0.8664\n",
      "Epoch 2 Batch 2650 Loss 0.4781 Accuracy 0.8664\n",
      "Epoch 2 Loss 0.4780 Accuracy 0.8664\n",
      "Time taken for 1 epoch: 3769.9993307590485 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-687bb7fa1edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2468\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2469\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "import time\n",
    "for epoch in range(2): #steps_per_epoch\n",
    "    start = time.time()\n",
    "  \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    " \n",
    "    for (batch, (que, img, tar)) in enumerate(dataset):\n",
    "        train_step(que, img, tar)\n",
    "    \n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_save_path = ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ques, img, out_dim):\n",
    "    ques_tensor = qu_token.texts_to_sequences(ques)\n",
    "    ques_tensor = tf.keras.preprocessing.sequence.pad_sequences(ques_tensor, padding='post')\n",
    "    ques_tensor.transpose((1, 0))\n",
    "    print('ques_tensor shape', ques_tensor.shape)\n",
    "    img_matrix = np.array(img_feature_struct[:,img_id_feature_map[img]])\n",
    "    img_matrix = img_matrix.reshape(out_dim, -1)\n",
    "    dec_input = tf.zeros((1, out_dim))\n",
    "\n",
    "    print('img shape:', img_matrix.shape, ' an shape:', dec_input.shape)\n",
    "    hidden = tf.zeros((1,d_model))\n",
    "    predictions,_ = transformer(ques_tensor,dec_input, img_matrix, hidden, False)\n",
    "    \n",
    "    pred = tf.squeeze(predictions, axis = 0)\n",
    "    pred_array = tf.random.categorical(pred, 1)\n",
    "    print(pred_array)\n",
    "    output =[]\n",
    "    for index_arr in pred_array:\n",
    "        index = index_arr.numpy()[0]\n",
    "        if (index > 0):\n",
    "            output.append(an_token.index_word[index])\n",
    "    \n",
    "    return output, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42952\n",
      "42952\n",
      "42952\n"
     ]
    }
   ],
   "source": [
    "#get test data\n",
    "test_questions_new = []\n",
    "test_img_ids_new = []\n",
    "test_answers_new = []\n",
    "\n",
    "for q, img, ans in zip(test_questions, test_img_ids, test_answers):\n",
    "    if ans in top_ans:\n",
    "        test_questions_new.append(q)\n",
    "        test_img_ids_new.append(img)\n",
    "        test_answers_new.append(ans)\n",
    "        \n",
    "print(len(test_questions_new))\n",
    "print(len(test_img_ids_new))\n",
    "print(len(test_answers_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ques_tensor shape (40, 1)\n",
      "img shape: (4, 1024)  an shape: (1, 4)\n",
      "in decoder : x shape (1, 4, 512)  layer: 0\n",
      "feature shape before: (4, 1024)\n",
      "feature shape: (4, 512)\n",
      "hidden_with_time_axis: (1, 1, 512)\n",
      "score: (1, 4, 512)\n",
      "attention_weights: (1, 4, 1)\n",
      "context_tensor bef: (1, 4, 512)\n",
      "context_tensor aft: (1, 4, 512)\n",
      "img_context shape before: (1, 4, 512)\n",
      "x shape (1, 4, 512)\n",
      "x_img_concate_shape: (1, 4, 1024)\n",
      "x_img_gru_shape: (1, 4, 512)\n",
      "x_img_out_shape: (1, 4, 512)\n",
      "in decoder: after layer, x shape: (1, 4, 512) decoder layer : 0\n",
      "in decoder : x shape (1, 4, 512)  layer: 1\n",
      "feature shape before: (4, 1024)\n",
      "feature shape: (4, 512)\n",
      "hidden_with_time_axis: (1, 1, 512)\n",
      "score: (1, 4, 512)\n",
      "attention_weights: (1, 4, 1)\n",
      "context_tensor bef: (1, 4, 512)\n",
      "context_tensor aft: (1, 4, 512)\n",
      "img_context shape before: (1, 4, 512)\n",
      "x shape (1, 4, 512)\n",
      "x_img_concate_shape: (1, 4, 1024)\n",
      "x_img_gru_shape: (1, 4, 512)\n",
      "x_img_out_shape: (1, 4, 512)\n",
      "in decoder: after layer, x shape: (1, 4, 512) decoder layer : 1\n",
      "tf.Tensor(\n",
      "[[2]\n",
      " [0]\n",
      " [0]\n",
      " [0]], shape=(4, 1), dtype=int64)\n",
      "questions for image 556840 is Was this picture taken around lunchtime?, \n",
      "pred answers is  no  real answer is  no\n",
      "prediction : tf.Tensor(\n",
      "[[[-1.7263707   9.705438    9.958572   ... -7.1053724  -7.380516\n",
      "   -8.635588  ]\n",
      "  [14.151441   -0.13040188  0.03705218 ... -5.333123   -5.1826067\n",
      "   -6.3498974 ]\n",
      "  [14.526688   -0.1663044  -0.04433489 ... -6.2741933  -6.0757093\n",
      "   -7.510912  ]\n",
      "  [14.987628   -0.45178148 -0.43358275 ... -5.7558584  -5.338031\n",
      "   -6.5150805 ]]], shape=(1, 4, 919), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred_word, pred = evaluate(test_questions_new[0], test_img_ids_new[0], an_tensor.shape[1])\n",
    "print('questions for image {} is {}, '.format(test_img_ids_new[0], test_questions_new[0]))\n",
    "print('pred answers is ', ' '.join(pred_word), ' real answer is ', test_answers_new[0])\n",
    "print('prediction :', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
